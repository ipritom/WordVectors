{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.constraints import nonneg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Here Wikipedia Movie Plots dataset have been used.<br>\n",
    "Dataset link: https://www.kaggle.com/jrobischon/wikipedia-movie-plots\n",
    "* You can fit your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "df = pd.read_csv('dataset/wiki_movie_plots_deduped.csv') #get it from Kaggle\n",
    "STOP_WORDS = set(nltk.corpus.stopwords.words('english')) \n",
    "df.Plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Plot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering and tokenizing function\n",
    "def sentence_to_wordlist(sentence, filters=\"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n?,।!‍.'0123456789০১২৩৪৫৬৭৮৯‘\\u200c–“”…‘\"):\n",
    "    translate_dict = dict((c, ' ') for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    wordlist = sentence.translate(translate_map).split()\n",
    "    return list(filter(lambda x: x not in STOP_WORDS, wordlist))\n",
    "\n",
    "#function for calculating word similarity\n",
    "def cosine_similarity(u, v):\n",
    "\n",
    "    '''Cosine similarity reflects the degree of similariy between u and v'''\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u,v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(np.power(u,2)))\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(np.power(v,2)))\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = np.divide(dot,norm_u*norm_v)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "#determine simularity between two words\n",
    "def word_similarity(word1,word2,weight,word_to_index,print_vec=False):\n",
    "    e1 = weight[word_to_index[word1]]\n",
    "    e2 = weight[word_to_index[word2]]\n",
    "    if print_vec:\n",
    "        print(word1,e1)\n",
    "        print(word2,e2)\n",
    "    return cosine_similarity(e1,e2)\n",
    "\n",
    "#retruns most similiar words\n",
    "def most_similiar(word,weight,word_to_index,count=5,print_vec=False):\n",
    "    e1 = weight[word_to_index[word]]\n",
    "    similarity = {}\n",
    "    for v in vocab:\n",
    "        if v is not word:\n",
    "            e2 = weight[word_to_index[v]]\n",
    "            similarity[v]=cosine_similarity(e1,e2)\n",
    "            \n",
    "    if print_vec:\n",
    "        print(word,e)\n",
    "        \n",
    "    sorted_similarity = sorted(similarity.items(), key=operator.itemgetter(1),reverse=True)      \n",
    "    return sorted_similarity[1:count+1]\n",
    "\n",
    "#function for vocab and indexing\n",
    "def word_idx_generate(text):\n",
    "    '''\n",
    "    --build vocabulary of unique words\n",
    "    --create dictionary for 'indext to word' and word to index'\n",
    "    '''\n",
    "    # build vocabulary of unique words\n",
    "    vocab = []\n",
    "    for sent in text:\n",
    "        for i in sent:\n",
    "            if i not in vocab:\n",
    "                vocab.append(i)\n",
    "                \n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2id[word] = i\n",
    "        id2word[i] = word\n",
    "    return vocab,word2id,id2word\n",
    "\n",
    "#dataset: CBOW Represantation\n",
    "def cbow(text,window=4):\n",
    "    '''Return dataset as one-hot encoded context(X) vs target(Y) words'''\n",
    "    X = []\n",
    "    Y = []\n",
    "    X_context = []\n",
    "    Y_target = []\n",
    "    V = len(vocab)\n",
    "    for sent in text:\n",
    "        border = len(sent)\n",
    "        end = window -1\n",
    "        start = 0\n",
    "        if len(sent)>=window:\n",
    "            while (border>end):\n",
    "                x_temp = []\n",
    "                x_word = [] #op\n",
    "                for i in range(start,end):\n",
    "                    index = word_to_index[sent[i]]\n",
    "                    temp = np.zeros((1,V))\n",
    "                    temp[0][index] = 1\n",
    "                    x_temp.append(temp)\n",
    "                    x_word.append(sent[i]) #op\n",
    "                    #print(i,sent[i],end,border) #op\n",
    "                index = word_to_index[sent[end]]\n",
    "                temp = np.zeros((1,V))\n",
    "                temp[0][index] = 1\n",
    "                Y.append(temp)\n",
    "                X.append(np.sum(x_temp,axis=0)) #summping context words\n",
    "                X_context.append(x_word)\n",
    "                Y_target.append(sent[end]) #op\n",
    "                start +=1\n",
    "                end +=1\n",
    "    \n",
    "    return X,Y,x_context,y_target\n",
    "\n",
    "#dataset: Skip-gram Represantation\n",
    "def skip_gram(text,word_to_index,window=4):\n",
    "    '''Return dataset as one-hot encoded context(X) vs target(Y) words'''\n",
    "    X = []\n",
    "    Y = []\n",
    "    X_context = []\n",
    "    Y_target = []\n",
    "    V = len(vocab)\n",
    "    for sent in text:\n",
    "        border = len(sent)\n",
    "        end = window-1\n",
    "        start = 0\n",
    "        if len(sent)>=window:\n",
    "            while (border>end):\n",
    "                index = word_to_index[sent[start]]\n",
    "                temp = np.zeros((1,V))\n",
    "                temp[0][index] = 1\n",
    "                X.append(temp)\n",
    "                y_temp = []\n",
    "                y_word = []\n",
    "                X_context.append(sent[start]) #op\n",
    "                for i in range(start+1,end+1):\n",
    "                    index = word_to_index[sent[i]]\n",
    "                    temp = np.zeros((1,V))\n",
    "                    temp[0][index] = 1\n",
    "                    y_temp.append(temp)\n",
    "                    y_word.append(sent[i])\n",
    "\n",
    "                Y.append(np.sum(y_temp,axis=0)) #summping context words\n",
    "                Y_target.append(y_word) #op\n",
    "                start +=1\n",
    "                end +=1\n",
    "       \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X,Y,X_context,Y_target\n",
    "\n",
    "def contex_target(X_context,Y_target):\n",
    "    '''prints context vs target words'''\n",
    "    for i,x in enumerate(X_context):\n",
    "        print(i,\"Context :\",x,'---> Target:',Y_target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset for word vectors\n",
    "We are taking first 200 plots for reducing time it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[:200] #selecting first 200 entry\n",
    "text = [sentence_to_wordlist(i.lower()) for i in text.Plot] #filtering and tokenizing plot text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, word2idx,idx2word = word_idx_generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y,X_context,Y_target = skip_gram(text,word2idx)\n",
    "print('X:',X.shape,'Y:',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have followed skip-gram model. Shape of X and Y shows, we have **18360 context word against 18360 target word sets.** <br>\n",
    "Here we can visualize **context word** Vs **target word** with *context_target()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contex_target(X_context,Y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping dataset to fit in neural network\n",
    "r = [X.shape[0],X.shape[2]]\n",
    "X = X.reshape(r)\n",
    "Y = Y.reshape(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=r[1], init= 'uniform' ,kernel_constraint=nonneg(), activation= 'sigmoid' ))\n",
    "model.add(Dense(r[1], init= 'uniform',kernel_constraint=nonneg(), activation= 'softmax' ))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = model.fit(X, Y, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity Analysis\n",
    "Now we will use the weight matrix of our Neural Network.<br>\n",
    "We have two weight metrices for two layers. We have taken 100 hidden units in hidden layer. So, for each word we have 100 dimentional vector.\n",
    "<br><br>\n",
    "Weights from both layers need to be summed for final vector.\n",
    "<br><br>\n",
    "**Here is an important thing:** Word vectors have less dimention than the one-hot encoding matrix. It reduce computation complexity in NLP task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1, biases = model.layers[0].get_weights()\n",
    "weight2, biases = model.layers[1].get_weights()\n",
    "weight = weight1 +weight2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_similarity('afternoon','dead',weight,word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(most_similiar('man',weight,word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing Past Results \n",
    "In this section weights from the first training article to reproduce the past result that has been discussed in the medium article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "#joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, biases = loaded_model.layers[0].get_weights()\n",
    "W2, biases = loaded_model.layers[1].get_weights()\n",
    "W = W1 +W2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(most_similiar('man',W,word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list((word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
